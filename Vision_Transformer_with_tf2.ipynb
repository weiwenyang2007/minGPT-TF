{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ashishpatel26/Vision-Transformer-Keras-Tensorflow-Pytorch-Examples/blob/main/Vision_Transformer_with_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original file is https://github.com/ashishpatel26/Vision-Transformer-Keras-Tensorflow-Pytorch-Examples/blob/main/Vision_Transformer_with_tf2.ipynb\n",
    "# Modify its trainer based on minigpt-tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "RrCqNMfrGNwN"
   },
   "outputs": [],
   "source": [
    "#!pip install einops\n",
    "  \n",
    "import math\n",
    "\n",
    "import six\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbgZNuWmGhLS",
    "outputId": "2e959630-268e-410d-ca00-22d8de9eab4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 28 22:45:02 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| 90%   64C    P8    46W / 350W |  23053MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n",
      "| 58%   47C    P8    35W / 350W |  23044MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wd1sZULgIxn4"
   },
   "source": [
    "### Vision Transformer\n",
    "\n",
    "![](https://raw.githubusercontent.com/kamalkraj/Vision-Transformer/main/vit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Ral9LvuZIJg4"
   },
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit.\n",
    "    This is a smoother version of the RELU.\n",
    "    Original paper: https://arxiv.org/abs/1606.08415\n",
    "    Args:\n",
    "        x: float Tensor to perform activation.\n",
    "    Returns:\n",
    "        `x` with the GELU activation applied.\n",
    "    \"\"\"\n",
    "    cdf = 0.5 * (1.0 + tf.tanh(\n",
    "        (math.sqrt(2 / math.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf\n",
    "\n",
    "\n",
    "def get_activation(identifier):\n",
    "    \"\"\"Maps a identifier to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n",
    "    It checks string first and if it is one of customized activation not in TF,\n",
    "    the corresponding activation will be returned. For non-customized activation\n",
    "    names and callable identifiers, always fallback to tf.keras.activations.get.\n",
    "    Args:\n",
    "        identifier: String name of the activation function or callable.\n",
    "    Returns:\n",
    "        A Python function corresponding to the activation function.\n",
    "    \"\"\"\n",
    "    if isinstance(identifier, six.string_types):\n",
    "        name_to_fn = {\"gelu\": gelu}\n",
    "        identifier = str(identifier).lower()\n",
    "        if identifier in name_to_fn:\n",
    "            return tf.keras.activations.get(name_to_fn[identifier])\n",
    "    return tf.keras.activations.get(identifier)\n",
    "\n",
    "\n",
    "class Residual(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "\n",
    "class PreNorm(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.fn = fn\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.fn(self.norm(x))\n",
    "\n",
    "\n",
    "class FeedForward(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential([tf.keras.layers.Dense(hidden_dim, activation=get_activation('gelu')),\n",
    "                                        tf.keras.layers.Dense(dim)])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, dim, heads = 8):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim ** -0.5\n",
    "\n",
    "        self.to_qkv = tf.keras.layers.Dense(dim * 3, use_bias=False)\n",
    "        self.to_out = tf.keras.layers.Dense(dim)\n",
    "\n",
    "        self.rearrange_qkv = Rearrange('b n (qkv h d) -> qkv b h n d', qkv = 3, h = self.heads)\n",
    "        self.rearrange_out = Rearrange('b h n d -> b n (h d)')\n",
    "\n",
    "    def call(self, x):\n",
    "        qkv = self.to_qkv(x)\n",
    "        qkv = self.rearrange_qkv(qkv)\n",
    "        q = qkv[0]\n",
    "        k = qkv[1]\n",
    "        v = qkv[2]\n",
    "\n",
    "        dots = tf.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "        attn = tf.nn.softmax(dots,axis=-1)\n",
    "\n",
    "        out = tf.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = self.rearrange_out(out)\n",
    "        out =  self.to_out(out)\n",
    "        return out\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, dim, depth, heads, mlp_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(depth):\n",
    "            layers.extend([\n",
    "                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n",
    "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n",
    "            ])\n",
    "        self.net = tf.keras.Sequential(layers)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ViT(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = channels * patch_size ** 2\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "        self.pos_embedding = self.add_weight(\"position_embeddings\",\n",
    "                                             shape=[num_patches + 1,\n",
    "                                                    dim],\n",
    "                                             initializer=tf.keras.initializers.RandomNormal(),\n",
    "                                             dtype=tf.float32)\n",
    "        self.patch_to_embedding = tf.keras.layers.Dense(dim)\n",
    "        self.cls_token = self.add_weight(\"cls_token\",\n",
    "                                         shape=[1,\n",
    "                                                1,\n",
    "                                                dim],\n",
    "                                         initializer=tf.keras.initializers.RandomNormal(),\n",
    "                                         dtype=tf.float32)\n",
    "\n",
    "        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
    "\n",
    "        self.to_cls_token = tf.identity\n",
    "\n",
    "        self.mlp_head = tf.keras.Sequential([tf.keras.layers.Dense(mlp_dim, activation=get_activation('gelu')),\n",
    "                                        tf.keras.layers.Dense(num_classes)])\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, img):\n",
    "        shapes = tf.shape(img)\n",
    "\n",
    "        x = self.rearrange(img)\n",
    "        x = self.patch_to_embedding(x)\n",
    "\n",
    "        cls_tokens = tf.broadcast_to(self.cls_token,(shapes[0],1,self.dim))\n",
    "        x = tf.concat((cls_tokens, x), axis=1)\n",
    "        x += self.pos_embedding\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "XP7ChizsKIFD"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 6e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1  # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = True\n",
    "    # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    warmup_tokens = 375e6\n",
    "    final_tokens = 260e9  # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = './checkpoings/ckpt_vision/minigpt.ckpt'\n",
    "    num_workers = 4  # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "js3WhD1gN30L"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.distribute.values import PerReplica\n",
    "from mingpt.optimization import AdamWeightDecay\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, model_config, train_dataset, train_dataset_len, test_dataset, test_dataset_len, config):\n",
    "        self.train_dataset = train_dataset.batch(config.batch_size)\n",
    "        self.train_dataset_len = train_dataset_len\n",
    "        self.test_dataset = test_dataset\n",
    "        self.test_dataset_len = None\n",
    "        self.test_dist_dataset = None\n",
    "        if self.test_dataset:\n",
    "            self.test_dataset = test_dataset.batch(config.batch_size)\n",
    "            self.test_dataset_len = test_dataset_len\n",
    "        self.config = config\n",
    "        self.tokens = 0\n",
    "        self.strategy = tf.distribute.OneDeviceStrategy(\"GPU:0\")\n",
    "        if len(tf.config.list_physical_devices('GPU')) > 1:\n",
    "            self.strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "        with self.strategy.scope():\n",
    "            self.model = model(**model_config)\n",
    "            #WA start: align to minigpt\n",
    "            #self.optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
    "            self.optimizer = AdamWeightDecay(learning_rate=self.lr_cosine_decay if config.lr_decay else config.learning_rate,\n",
    "                                             weight_decay_rate=config.weight_decay,\n",
    "                                             beta_1=config.betas[0], beta_2=config.betas[1],\n",
    "                                             gradient_clip_norm=config.grad_norm_clip,\n",
    "                                             exclude_from_weight_decay=['layer_normalization', 'bias'])            \n",
    "            #WA end\n",
    "            \n",
    "            self.cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction=tf.keras.losses.Reduction.NONE)\n",
    "            self.train_dist_dataset = self.strategy.experimental_distribute_dataset(self.train_dataset)\n",
    "            if self.test_dataset:\n",
    "                self.test_dist_dataset = self.strategy.experimental_distribute_dataset(self.test_dataset)\n",
    "\n",
    "    def save_checkpoints(self):\n",
    "        if self.config.ckpt_path is not None:\n",
    "            #print('The gpt train model weight is saved to checkpoints:'+self.config.ckpt_path)\n",
    "            self.model.save_weights(self.config.ckpt_path)\n",
    "        else:\n",
    "            print('config.ckpt_path is not set, the gpt train model weight will not be saved')\n",
    "\n",
    "    def load_checkpoints(self):\n",
    "        if self.config.ckpt_path is not None:\n",
    "            print('The gpt train model weight is load from checkpoints:'+self.config.ckpt_path)\n",
    "            self.model.load_weights(self.config.ckpt_path)\n",
    "\n",
    "    def lr_cosine_decay(self):\n",
    "        if self.tokens == 0:\n",
    "            lr_mult = 1\n",
    "        elif self.tokens < self.config.warmup_tokens:\n",
    "            lr_mult = float(self.tokens) / float(max(1, self.config.warmup_tokens))\n",
    "        else:\n",
    "            progress = float(self.tokens - self.config.warmup_tokens) / float(max(1, self.config.final_tokens - self.config.warmup_tokens))\n",
    "            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "        lr = self.config.learning_rate * lr_mult\n",
    "        return lr\n",
    "\n",
    "    \n",
    "    #must call after the model.build, so call it after train()\n",
    "    def display_model(self):        \n",
    "        self.model.summary()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        train_loss_metric = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
    "        test_loss_metric = tf.keras.metrics.Mean('testing_loss', dtype=tf.float32)\n",
    "\n",
    "        train_accuracy = tf.keras.metrics.Accuracy('training_accuracy', dtype=tf.float32)\n",
    "        test_accuracy = tf.keras.metrics.Accuracy('testing_accuracy', dtype=tf.float32)\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(dist_inputs):\n",
    "\n",
    "            def step_fn(inputs):\n",
    "\n",
    "                X, Y = inputs\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                # training=True is only needed if there are layers with different\n",
    "                # behavior during training versus inference (e.g. Dropout).\n",
    "                    logits = self.model(X,training=True)\n",
    "                    num_labels = tf.shape(logits)[-1]\n",
    "                    label_mask = tf.math.logical_not(Y < 0)\n",
    "                    label_mask = tf.reshape(label_mask,(-1,))\n",
    "                    logits = tf.reshape(logits,(-1,num_labels))\n",
    "                    logits_masked = tf.boolean_mask(logits,label_mask)\n",
    "                    label_ids = tf.reshape(Y,(-1,))\n",
    "                    label_ids_masked = tf.boolean_mask(label_ids,label_mask)\n",
    "                    cross_entropy = self.cce(label_ids_masked, logits_masked)\n",
    "                    loss = tf.reduce_sum(cross_entropy) * (1.0 / self.config.batch_size)\n",
    "                    y_pred = tf.argmax(tf.nn.softmax(logits,axis=-1),axis=-1)\n",
    "                    train_accuracy.update_state(tf.squeeze(Y),y_pred)\n",
    "\n",
    "                grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(list(zip(grads, self.model.trainable_variables)))\n",
    "                return cross_entropy\n",
    "\n",
    "            per_example_losses = self.strategy.run(step_fn, args=(dist_inputs,))\n",
    "            sum_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_example_losses, axis=0)\n",
    "            mean_loss = sum_loss / self.config.batch_size\n",
    "            return mean_loss\n",
    "\n",
    "        @tf.function\n",
    "        def test_step(dist_inputs):\n",
    "\n",
    "            def step_fn(inputs):\n",
    "\n",
    "                X, Y = inputs\n",
    "                # training=True is only needed if there are layers with different\n",
    "                # behavior during training versus inference (e.g. Dropout).\n",
    "                logits = self.model(X,training=False)\n",
    "                num_labels = tf.shape(logits)[-1]\n",
    "                label_mask = tf.math.logical_not(Y < 0)\n",
    "                label_mask = tf.reshape(label_mask,(-1,))\n",
    "                logits = tf.reshape(logits,(-1,num_labels))\n",
    "                logits_masked = tf.boolean_mask(logits,label_mask)\n",
    "                label_ids = tf.reshape(Y,(-1,))\n",
    "                label_ids_masked = tf.boolean_mask(label_ids,label_mask)\n",
    "                cross_entropy = self.cce(label_ids_masked, logits_masked)\n",
    "                loss = tf.reduce_sum(cross_entropy) * (1.0 / self.config.batch_size)\n",
    "                y_pred = tf.argmax(tf.nn.softmax(logits,axis=-1),axis=-1)\n",
    "                test_accuracy.update_state(tf.squeeze(Y),y_pred)\n",
    "\n",
    "                return cross_entropy\n",
    "\n",
    "            per_example_losses = self.strategy.run(step_fn, args=(dist_inputs,))\n",
    "            sum_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_example_losses, axis=0)\n",
    "            mean_loss = sum_loss / self.config.batch_size\n",
    "            return mean_loss\n",
    "\n",
    "        train_pb_max_len = math.ceil(float(self.train_dataset_len)/float(self.config.batch_size))\n",
    "        test_pb_max_len = math.ceil(float(self.test_dataset_len)/float(self.config.batch_size)) if self.test_dataset else None\n",
    "\n",
    "        epoch_bar = master_bar(range(self.config.max_epochs))\n",
    "        with self.strategy.scope():\n",
    "            for epoch in epoch_bar:\n",
    "                for inputs in progress_bar(self.train_dist_dataset,total=train_pb_max_len,parent=epoch_bar):\n",
    "                    loss = train_step(inputs)\n",
    "                    #WA start:\n",
    "                    #original code is:\n",
    "                    #self.tokens += tf.reduce_sum(tf.cast(inputs[1]>=0,tf.int32)).numpy()\n",
    "                    labels = inputs[-1]\n",
    "                    if isinstance(labels, PerReplica):\n",
    "                        labels = tf.concat(labels.values, axis=0)\n",
    "                    self.tokens += tf.reduce_sum(tf.cast(labels>=0,tf.int32)).numpy()\n",
    "                    #WA end\n",
    "                    train_loss_metric(loss)\n",
    "                    #epoch_bar.child.comment = f'training loss : {train_loss_metric.result()}'\n",
    "                    epoch_bar.child.comment = f'training loss : {train_loss_metric.result()} lr : {self.optimizer._decayed_lr(tf.float32):e}'\n",
    "                print(f\"epoch {epoch+1}: train loss {train_loss_metric.result():.5f}. train accuracy {train_accuracy.result():.5f}\")\n",
    "                #print(f\"epoch {epoch+1}: train loss {train_loss_metric.result():.5f}. lr {self.optimizer._decayed_lr(tf.float32):e}\")\n",
    "                \n",
    "                train_loss_metric.reset_states()\n",
    "                train_accuracy.reset_states()\n",
    "\n",
    "                if self.test_dist_dataset:\n",
    "                    for inputs in progress_bar(self.test_dist_dataset,total=test_pb_max_len,parent=epoch_bar):\n",
    "                        loss = test_step(inputs)\n",
    "                        test_loss_metric(loss)\n",
    "                        epoch_bar.child.comment = f'testing loss : {test_loss_metric.result()}'\n",
    "                    print(f\"epoch {epoch+1}: test loss {test_loss_metric.result():.5f}. test accuracy {test_accuracy.result():.5f}\")\n",
    "                    #print(f\"epoch {epoch+1}: test loss {test_loss_metric.result():.5f}.\")\n",
    "                    test_loss_metric.reset_states()\n",
    "                    test_accuracy.reset_states()\n",
    "\n",
    "                self.save_checkpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSHQHbijgjww",
    "outputId": "dc8ee257-b6b7-4d99-b1b7-415f5d8edb4c"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "W4GhXMcHg1ZM"
   },
   "outputs": [],
   "source": [
    "train_images = tf.cast(train_images.reshape((-1, 3, 32, 32)),dtype=tf.float32)\n",
    "test_images = tf.cast(test_images.reshape((-1, 3, 32, 32)),dtype=tf.float32)\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "EtBuVnoIhCBZ"
   },
   "outputs": [],
   "source": [
    "train_x = tf.data.Dataset.from_tensor_slices(train_images,)\n",
    "train_y = tf.data.Dataset.from_tensor_slices(train_labels)\n",
    "train_dataset = tf.data.Dataset.zip((train_x,train_y))\n",
    "test_x = tf.data.Dataset.from_tensor_slices(test_images)\n",
    "test_y = tf.data.Dataset.from_tensor_slices(test_labels)\n",
    "test_dataset = tf.data.Dataset.zip((test_x,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "779iRot8hGl0"
   },
   "outputs": [],
   "source": [
    "tconf = TrainerConfig(max_epochs=30, batch_size=1024, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "EGfcfoC7hJ1e"
   },
   "outputs": [],
   "source": [
    "# sample model config.\n",
    "model_config = {\"image_size\":32,\n",
    "                \"patch_size\":4,\n",
    "                \"num_classes\":10,\n",
    "                \"dim\":64,\n",
    "                \"depth\":3,\n",
    "                \"heads\":4,\n",
    "                \"mlp_dim\":128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "LLK2t2puhNJ5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(ViT, model_config, train_dataset, len(train_images), test_dataset, len(test_images), tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "axKI4hQshPz-",
    "outputId": "003b1ecd-feea-470c-c105-3c2da54d852f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss 2.15952. train accuracy 0.20862\n",
      "epoch 1: test loss 1.92614. test accuracy 0.29400\n",
      "epoch 2: train loss 1.85471. train accuracy 0.32528\n",
      "epoch 2: test loss 1.69283. test accuracy 0.36910\n",
      "epoch 3: train loss 1.68176. train accuracy 0.38676\n",
      "epoch 3: test loss 1.59090. test accuracy 0.40400\n",
      "epoch 4: train loss 1.59509. train accuracy 0.41548\n",
      "epoch 4: test loss 1.52649. test accuracy 0.43140\n",
      "epoch 5: train loss 1.52049. train accuracy 0.44486\n",
      "epoch 5: test loss 1.49474. test accuracy 0.44750\n",
      "epoch 6: train loss 1.46045. train accuracy 0.46772\n",
      "epoch 6: test loss 1.44570. test accuracy 0.46710\n",
      "epoch 7: train loss 1.42474. train accuracy 0.48076\n",
      "epoch 7: test loss 1.41442. test accuracy 0.47810\n",
      "epoch 8: train loss 1.38680. train accuracy 0.49598\n",
      "epoch 8: test loss 1.38747. test accuracy 0.48710\n",
      "epoch 9: train loss 1.35297. train accuracy 0.51086\n",
      "epoch 9: test loss 1.37256. test accuracy 0.49360\n",
      "epoch 10: train loss 1.32319. train accuracy 0.52072\n",
      "epoch 10: test loss 1.34991. test accuracy 0.50140\n",
      "epoch 11: train loss 1.29547. train accuracy 0.53194\n",
      "epoch 11: test loss 1.33489. test accuracy 0.50790\n",
      "epoch 12: train loss 1.27159. train accuracy 0.53968\n",
      "epoch 12: test loss 1.32765. test accuracy 0.51180\n",
      "epoch 13: train loss 1.24444. train accuracy 0.55018\n",
      "epoch 13: test loss 1.31498. test accuracy 0.51930\n",
      "epoch 14: train loss 1.21952. train accuracy 0.55976\n",
      "epoch 14: test loss 1.30905. test accuracy 0.52180\n",
      "epoch 15: train loss 1.19949. train accuracy 0.56724\n",
      "epoch 15: test loss 1.29808. test accuracy 0.52220\n",
      "epoch 16: train loss 1.18321. train accuracy 0.57536\n",
      "epoch 16: test loss 1.29235. test accuracy 0.52490\n",
      "epoch 17: train loss 1.16649. train accuracy 0.58086\n",
      "epoch 17: test loss 1.29893. test accuracy 0.52680\n",
      "epoch 18: train loss 1.14806. train accuracy 0.58726\n",
      "epoch 18: test loss 1.30106. test accuracy 0.52650\n",
      "epoch 19: train loss 1.12819. train accuracy 0.59436\n",
      "epoch 19: test loss 1.30106. test accuracy 0.53360\n",
      "epoch 20: train loss 1.11285. train accuracy 0.60082\n",
      "epoch 20: test loss 1.30749. test accuracy 0.52840\n",
      "epoch 21: train loss 1.10255. train accuracy 0.60446\n",
      "epoch 21: test loss 1.28661. test accuracy 0.53610\n",
      "epoch 22: train loss 1.09399. train accuracy 0.60652\n",
      "epoch 22: test loss 1.29922. test accuracy 0.53210\n",
      "epoch 23: train loss 1.07443. train accuracy 0.61356\n",
      "epoch 23: test loss 1.30743. test accuracy 0.53140\n",
      "epoch 24: train loss 1.05293. train accuracy 0.62340\n",
      "epoch 24: test loss 1.31343. test accuracy 0.53060\n",
      "epoch 25: train loss 1.04004. train accuracy 0.62626\n",
      "epoch 25: test loss 1.32764. test accuracy 0.52730\n",
      "epoch 26: train loss 1.04003. train accuracy 0.62702\n",
      "epoch 26: test loss 1.32757. test accuracy 0.52500\n",
      "epoch 27: train loss 1.02984. train accuracy 0.63000\n",
      "epoch 27: test loss 1.31937. test accuracy 0.53050\n",
      "epoch 28: train loss 1.01015. train accuracy 0.63834\n",
      "epoch 28: test loss 1.32896. test accuracy 0.53240\n",
      "epoch 29: train loss 0.99390. train accuracy 0.64298\n",
      "epoch 29: test loss 1.32927. test accuracy 0.53310\n",
      "epoch 30: train loss 0.98742. train accuracy 0.64414\n",
      "epoch 30: test loss 1.34368. test accuracy 0.52820\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNw6eZhArvwuJ/iH9k/WUFO",
   "include_colab_link": true,
   "name": "Vision Transformer with tf2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
